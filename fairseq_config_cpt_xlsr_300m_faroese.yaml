
# @package _group_

common:
  fp16: true
  log_format: json
  log_interval: 200
  seed: 1337
  tensorboard_logdir: /work/Model/logs/tensorboard/faroese
  wandb_project: fairseq

checkpoint:
  save_dir: /work/Model/checkpoints/faroese
  finetune_from_model: /work/Model/pre_trained_model/xlsr2_300m.pt
  save_interval: 1

distributed_training:
  distributed_world_size: 2
  ddp_backend: legacy_ddp
  find_unused_parameters: true

task:
  _name: audio_pretraining
  data: /work/Model/manifest
  labels: "ltr"
  sample_rate: 16000
  max_sample_size: 320000
  min_sample_size: 320000
  normalize: false

dataset:
  num_workers: 24
  train_subset: train
  valid_subset: valid
  max_tokens: 4800000

criterion:
  _name: wav2vec
  infonce: true
  log_keys:
    - "loss"
    - "target_var"
    - "code_perplexity"
    - "prob_perplexity"

optimization:
  max_update: 80000       # 1M updates was used in the original paper (https://arxiv.org/pdf/2111.09296). Update warmup_updates accordingly
  lr: [0.0002]            # optimal LR for Fareose
  update_freq: [2]

optimizer:
  _name: adam
  adam_betas: (0.9, 0.98)
  adam_eps: 1e-06
  weight_decay: 0.01

lr_scheduler:
  _name: polynomial_decay
  warmup_updates: 8000     # 10% of max_update. Original paper (https://arxiv.org/pdf/2111.09296)

model:
  _name: wav2vec2
  extractor_mode: layer_norm
  encoder_layers: 24
  encoder_embed_dim: 1024
  encoder_ffn_embed_dim: 4096
  encoder_attention_heads: 16
  activation_fn: gelu
  layer_type: transformer
  final_dim: 768
  latent_dim: 768
  
  layer_norm_first: true
  conv_feature_layers: '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512, 2, 2)] * 2'
  conv_bias: true
  feature_grad_mult: 0.0    # Org. setting was 0.1. Setting it to 0.0 will freeze the CNN layers

  # Masking
  mask_length: 10
  mask_prob: 0.65
  mask_selection: static

  # Quantization
  quantize_targets: true
  latent_vars: 320
  latent_groups: 2
  latent_temp: [2.0, 0.5, 0.999995]

  # Dropout
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  encoder_layerdrop: 0.1
  dropout_input: 0.1